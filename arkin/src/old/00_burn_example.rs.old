use burn::config::Config;
use burn::data::dataloader::batcher::Batcher;
use burn::data::dataset::Dataset;
use burn::data::dataset::InMemDataset;
use burn::module::Module;
use burn::nn::loss::MseLoss;
use burn::nn::loss::Reduction;
use burn::nn::Dropout;
use burn::nn::DropoutConfig;
use burn::nn::Linear;
use burn::nn::LinearConfig;
use burn::nn::Relu;
use burn::optim::AdamConfig;
use burn::prelude::Backend;
use burn::record::CompactRecorder;
use burn::record::Recorder;
use burn::tensor::backend::AutodiffBackend;
use burn::tensor::Tensor;
use burn::train::metric::CpuMemory;
use burn::train::metric::LearningRateMetric;
use burn::train::metric::LossMetric;
use burn::train::LearnerBuilder;
use burn::train::RegressionOutput;
use burn::train::TrainOutput;
use burn::train::TrainStep;
use burn::train::ValidStep;
use burn::{
    backend::{
        wgpu::{Wgpu, WgpuDevice},
        Autodiff,
    },
    data::dataloader::DataLoaderBuilder,
};

#[derive(Debug, Clone)]
struct DataPoint {
    features: [f32; 2],
    targets: [f32; 1],
}

struct TimeSeriesDataset {
    data: InMemDataset<DataPoint>,
}

impl TimeSeriesDataset {
    pub fn train() -> Self {
        Self::new(100000)
    }

    pub fn test() -> Self {
        Self::new(1000)
    }

    fn new(size: usize) -> Self {
        // Create some dummy data
        let data = (1..size)
            .into_iter()
            .map(|i| DataPoint {
                features: [1.0 + i as f32, 2.0 - i as f32],
                targets: [i as f32],
            })
            .collect::<Vec<_>>();

        // Regularize the data
        let mean_0 = data.iter().map(|point| point.features[0]).sum::<f32>() / size as f32;
        let std_0 = data.iter().map(|point| point.features[0]).sum::<f32>() / size as f32;

        let mean_1 = data.iter().map(|point| point.features[1]).sum::<f32>() / size as f32;
        let std_1 = data.iter().map(|point| point.features[1]).sum::<f32>() / size as f32;

        let mean_targets = data.iter().map(|point| point.targets[0]).sum::<f32>() / size as f32;
        let std_targets = data.iter().map(|point| point.targets[0]).sum::<f32>() / size as f32;
        info!("Mean 0: {} Std 0: {}", mean_0, std_0);
        info!("Mean 1: {} Std 1: {}", mean_1, std_1);
        info!("Mean Targets: {} Std Targets: {}", mean_targets, std_targets);

        let data = data
            .into_iter()
            .map(|point| DataPoint {
                features: [(point.features[0] - mean_0) / std_0, (point.features[1] - mean_1) / std_1],
                targets: [(point.targets[0] - mean_targets) / std_targets],
            })
            .collect::<Vec<_>>();

        Self {
            data: InMemDataset::new(data),
        }
    }
}

impl Dataset<DataPoint> for TimeSeriesDataset {
    fn len(&self) -> usize {
        self.data.len()
    }
    fn get(&self, idx: usize) -> Option<DataPoint> {
        self.data.get(idx)
    }
}

#[derive(Clone)]
pub struct TimeSeriesBatcher<B: Backend> {
    device: B::Device,
}

impl<B: Backend> TimeSeriesBatcher<B> {
    pub fn new(device: B::Device) -> Self {
        Self { device }
    }
}

#[derive(Clone, Debug)]
pub struct TimeSeriesBatch<B: Backend> {
    features: Tensor<B, 2>, // [batch_size, num_features]
    targets: Tensor<B, 2>,  // [batch_size, num_targets]
}

impl<B: Backend> Batcher<DataPoint, TimeSeriesBatch<B>> for TimeSeriesBatcher<B> {
    fn batch(&self, items: Vec<DataPoint>) -> TimeSeriesBatch<B> {
        let batch_size = items.len();
        let num_features = items[0].features.len();
        let num_targets = items[0].targets.len();

        let mut features = Vec::with_capacity(batch_size * num_features);
        let mut targets = Vec::with_capacity(batch_size);

        items.iter().for_each(|point| {
            features.extend(point.features);
            targets.extend(point.targets);
        });

        // Create tensors for features and targets
        let features_tensor = Tensor::<B, 1>::from_floats(features.as_slice(), &self.device);
        let targets_tensor = Tensor::<B, 1>::from_floats(targets.as_slice(), &self.device);

        // Reshape the features tensor to [batch_size, num_features]
        let features_tensor = features_tensor.reshape([batch_size, num_features]);
        // Reshape the targets tensor to [batch_size, num_targets]
        let targets_tensor = targets_tensor.reshape([batch_size, num_targets]);

        // info!("Features: {}", features_tensor.to_string());
        // info!("Targets: {}", targets_tensor.to_string());

        TimeSeriesBatch {
            features: features_tensor,
            targets: targets_tensor,
        }
    }
}

// #[derive(Clone, Debug)]
// pub struct TimeSeriesBatch<B: Backend> {
//     features: Tensor<B, 3>, // [batch_size, seq_len, num_features]
//     targets: Tensor<B, 2>,  // [batch_size, seq_len]
// }

// impl<B: Backend> Batcher<Vec<DataPoint>, TimeSeriesBatch<B>> for TimeSeriesBatcher<B> {
//     fn batch(&self, items: Vec<Vec<DataPoint>>) -> TimeSeriesBatch<B> {
//         let batch_size = items.len();
//         let seq_len = items[0].len();
//         let num_features = items[0][0].features.len();

//         let mut features = Vec::with_capacity(batch_size * seq_len * num_features);
//         let mut targets = Vec::with_capacity(batch_size * seq_len);

//         items.iter().for_each(|sequence| {
//             sequence.iter().for_each(|point| {
//                 features.extend(point.features);
//                 targets.push(point.targets[0]);
//             });
//         });

//         // Create tensors for features and targets
//         let features_tensor = Tensor::<B, 1>::from_floats(features.as_slice(), &self.device);
//         let targets_tensor = Tensor::<B, 1>::from_floats(targets.as_slice(), &self.device);

//         // Reshape the features tensor to [batch_size, seq_len, num_features]
//         let features_tensor = features_tensor.reshape([batch_size, seq_len, num_features]);
//         // Reshape the targets tensor to [batch_size, num_targets]
//         let targets_tensor = targets_tensor.reshape([batch_size, seq_len]);

//         TimeSeriesBatch {
//             features: features_tensor,
//             targets: targets_tensor,
//         }
//     }
// }

#[derive(Config, Debug)]
struct TimeSeriesModelConfig {
    num_features: usize,
    num_outputs: usize,
    #[config(default = "64")]
    hidden_size: usize,
    #[config(default = "0.5")]
    dropout: f64,
}

impl TimeSeriesModelConfig {
    /// Returns the initialized model.
    pub fn init<B: Backend>(&self, device: &B::Device) -> TimeSeriesModel<B> {
        TimeSeriesModel {
            input: LinearConfig::new(self.num_features, self.hidden_size).init(device),
            hidden1: LinearConfig::new(self.hidden_size, self.hidden_size).init(device),
            hidden2: LinearConfig::new(self.hidden_size, self.hidden_size).init(device),
            output: LinearConfig::new(self.hidden_size, self.num_outputs).init(device),
            activation: Relu::new(),
            dropout: DropoutConfig::new(self.dropout).init(),
        }
    }
}

#[derive(Module, Debug)]
struct TimeSeriesModel<B: Backend> {
    input: Linear<B>,
    hidden1: Linear<B>,
    hidden2: Linear<B>,
    output: Linear<B>,
    dropout: Dropout,
    activation: Relu,
}

impl<B: Backend> TimeSeriesModel<B> {
    /// # Shapes
    ///   - Features [batch_size, seq_len, num_features]
    ///   - Output [batch_size, seq_len]
    pub fn forward(&self, features: Tensor<B, 2>) -> Tensor<B, 2> {
        let [batch_size, num_features] = features.dims();

        // Create a channel at the second dimension.
        let x = features.reshape([batch_size, num_features]);

        let x = self.input.forward(x); // [batch_size, 8, _, _]
        let x = self.activation.forward(x);
        let x = self.hidden1.forward(x); // [batch_size, 16, _, _]
        let x = self.dropout.forward(x);
        let x = self.activation.forward(x);
        let x = self.hidden2.forward(x); // [batch_size, 16, _, _]
        let x = self.dropout.forward(x);
        let x = self.activation.forward(x);

        let x = self.output.forward(x); // [batch_size, 1, _, _]

        x.reshape([batch_size, 1])
    }

    pub fn forward_loss(&self, features: Tensor<B, 2>, targets: Tensor<B, 2>) -> RegressionOutput<B> {
        let prediction = self.forward(features);
        let loss = MseLoss::default().forward(prediction.clone(), targets.clone(), Reduction::Auto);
        RegressionOutput::new(loss, prediction, targets)
    }
}

impl<B: AutodiffBackend> TrainStep<TimeSeriesBatch<B>, RegressionOutput<B>> for TimeSeriesModel<B> {
    fn step(&self, batch: TimeSeriesBatch<B>) -> TrainOutput<RegressionOutput<B>> {
        let ouput = self.forward_loss(batch.features, batch.targets);
        TrainOutput::new(self, ouput.loss.backward(), ouput)
    }
}

impl<B: Backend> ValidStep<TimeSeriesBatch<B>, RegressionOutput<B>> for TimeSeriesModel<B> {
    fn step(&self, batch: TimeSeriesBatch<B>) -> RegressionOutput<B> {
        self.forward_loss(batch.features, batch.targets)
    }
}

#[derive(Config)]
pub struct TrainingConfig {
    model: TimeSeriesModelConfig,
    optimizer: AdamConfig,
    #[config(default = 10)]
    num_epochs: usize,
    #[config(default = 4)]
    batch_size: usize,
    #[config(default = 4)]
    num_workers: usize,
    #[config(default = 1.0e-3)]
    learning_rate: f64,
}

fn create_artifact_dir(artifact_dir: &str) {
    // Remove existing artifacts before to get an accurate learner summary
    std::fs::remove_dir_all(artifact_dir).ok();
    std::fs::create_dir_all(artifact_dir).ok();
}

pub fn train<B: AutodiffBackend>(artifact_dir: &str, config: TrainingConfig, device: B::Device) {
    create_artifact_dir(artifact_dir);
    config
        .save(format!("{artifact_dir}/config.json"))
        .expect("Config should be saved successfully");

    let batcher_train = TimeSeriesBatcher::<B>::new(device.clone());
    let batcher_valid = TimeSeriesBatcher::<B::InnerBackend>::new(device.clone());

    let dataloader_train = DataLoaderBuilder::new(batcher_train)
        .batch_size(config.batch_size)
        .num_workers(config.num_workers)
        .build(TimeSeriesDataset::train());

    let dataloader_valid = DataLoaderBuilder::new(batcher_valid)
        .batch_size(config.batch_size)
        .num_workers(config.num_workers)
        .build(TimeSeriesDataset::test());

    let learner = LearnerBuilder::new(artifact_dir)
        .metric_train_numeric(LossMetric::new())
        .metric_valid_numeric(LossMetric::new())
        .metric_train_numeric(LearningRateMetric::new())
        .metric_valid_numeric(LearningRateMetric::new())
        .metric_train_numeric(CpuMemory::new())
        .metric_valid_numeric(CpuMemory::new())
        .with_file_checkpointer(CompactRecorder::new())
        .devices(vec![device.clone()])
        .num_epochs(config.num_epochs)
        .summary()
        .build(config.model.init::<B>(&device), config.optimizer.init(), config.learning_rate);

    let model_trained = learner.fit(dataloader_train, dataloader_valid);

    model_trained
        .save_file(format!("{artifact_dir}/model"), &CompactRecorder::new())
        .expect("Trained model should be saved successfully");
}

fn infer<B: Backend>(artifact_dir: &str, device: B::Device, item: DataPoint) {
    let config =
        TrainingConfig::load(format!("{artifact_dir}/config.json")).expect("Config should exist for the model");
    let record = CompactRecorder::new()
        .load(format!("{artifact_dir}/model").into(), &device)
        .expect("Trained model should exist");

    let model = config.model.init::<B>(&device).load_record(record);

    let batcher = TimeSeriesBatcher::new(device);
    let batch = batcher.batch(vec![item.clone()]);
    let output = model.forward(batch.features);
    let predicted = output.into_scalar();

    info!("Predicted {} Expected {}", predicted, item.targets[0]);
}

fn main() {
    type MyBackend = Wgpu<f32, i32>;
    type MyAutoDiffBackend = Autodiff<MyBackend>;

    let device = WgpuDevice::default();
    let artifact_dir = "/tmp/guide";

    // Train the model
    train::<MyAutoDiffBackend>(
        artifact_dir,
        TrainingConfig::new(
            TimeSeriesModelConfig::new(2, 1).with_dropout(0.2).with_hidden_size(16),
            AdamConfig::new(),
        )
        .with_batch_size(1024)
        .with_learning_rate(1.0e-3)
        .with_num_epochs(200)
        .with_num_workers(16),
        device.clone(),
    );

    infer::<MyBackend>(
        artifact_dir,
        device.clone(),
        DataPoint {
            features: [6.0, -4.0],
            targets: [5.0],
        },
    );

    // Print the model [TEST]
    // let model = TimeSeriesModelConfig::new(FEATURE_SIZE, TARGET_SIZE)
    //     .with_dropout(0.25)
    //     .init::<MyBackend>(&device);
    // info!("{}", model);

    // // TEST DATALOADER
    // // Create the dataset and dataloader
    // let batcher: TimeSeriesBatcher<Autodiff<Wgpu>> = TimeSeriesBatcher::new(device.clone());
    // let dataloader_train = DataLoaderBuilder::new(batcher)
    //     .batch_size(BATCH_SIZE)
    //     .num_workers(WORKER_COUNT)
    //     .build(TimeSeriesDataset::train());

    // // Iterate over the dataloader
    // let mut iter = dataloader_train.iter();
    // while let Some(data) = iter.next() {
    //     info!("Features: {}", data.features.to_string());
    //     info!("Targets: {}", data.targets.to_string());
    // }
}
